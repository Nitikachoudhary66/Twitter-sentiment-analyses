# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eWqzZ1esj0XAqTtMW9aLT_iGsKGQ8erE
"""

from google.colab import drive
drive.mount('/content/drive')

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import re
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report

# Load dataset
df = pd.read_csv('/content/drive/MyDrive/twitter_training.csv', header=None, index_col=[0])
df = df[[2, 3]].reset_index(drop=True)
df.columns = ['sentiment', 'text']
df

# Check for missing values and drop them
df.isnull().sum()
df.dropna(inplace=True)
df

# prompt: check duplicacy

# Check for duplicate rows
duplicate_rows = df[df.duplicated()]
print(f"Number of duplicate rows: {len(duplicate_rows)}")

# Display duplicate rows (optional)
print("Duplicate Rows:")
duplicate_rows

# Remove duplicate rows (optional)
df.drop_duplicates(inplace=True)
print(f"Shape after removing duplicates: {df.shape}")

# Define custom cleaning functions
'''def remove_urls(text):
    return re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

def remove_html_tags(text):
    return re.sub(r'<.*?>', '', text)

def remove_special_chars(text):
    return re.sub(r'[^a-zA-Z\s]', '', text)

def remove_retweet_prefix(text):
    return re.sub(r'^RT\s+', '', text)'''

'''!pip install nltk
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

# ... (rest of your imports) ...
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords


# Handle neutral sentiments explicitly
neutral_keywords =neutral_keywords = neutral_keywords = ["okay", "alright", "decent", "average", "indifferent", "neither good nor bad", "not sure", "pretty average", "nothing special", "meh", "it's fine"]

# Add more keywords here...
def label_neutral(text):
    for word in neutral_keywords:
        if word in text:
            return "neutral"
    return None

df['sentiment'] = df.apply(lambda row: label_neutral(row['text']) if label_neutral(row['text']) else row['sentiment'], axis=1)
df['text'] = df['text'].apply(handle_negation)
def handle_negation(text):
    tokens = word_tokenize(text)
    negation_words = ["do not", "would not", "could not", "not", "no", "never", "n't", "wouldn't", "shouldn't", "couldn't", "won't", "can't"]
    negated_tokens = []
    negation_flag = False
    negation_phrase = []

    for token in tokens:
        if any(neg in token for neg in negation_words):
            negation_flag = True  # Start tagging the negated part
            negation_phrase = [token]
        elif negation_flag:
            negation_phrase.append(token)
            if token in [".", "!", "?", ";", ","]:  # End of the negated phrase
                negation_flag = False
                negated_tokens.append("NEG_" + " ".join(negation_phrase))
                negation_phrase = []
        else:
            negated_tokens.append(token)

    return " ".join(negated_tokens)'''

# Apply text cleaning
'''df['text'] = df['text'].apply(lambda x: x.lower())
df['text'] = df['text'].apply(remove_urls)
df['text'] = df['text'].apply(remove_html_tags)
df['text'] = df['text'].apply(remove_special_chars)
df['text'] = df['text'].apply(remove_retweet_prefix)'''

# Define custom cleaning functions
import re

def remove_urls(text):
    return re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

def remove_html_tags(text):
    return re.sub(r'<.*?>', '', text)

def remove_special_chars(text):
    return re.sub(r'[^a-zA-Z\s]', '', text)

def remove_retweet_prefix(text):
    return re.sub(r'^RT\s+', '', text)


# Install necessary libraries and download NLTK resources (run once)
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

# Import necessary libraries
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Define function to handle negations
def handle_negation(text):
    tokens = word_tokenize(text)
    negation_words = ["do not", "would not", "could not", "not", "no", "never", "n't", "wouldn't", "shouldn't", "couldn't", "won't", "can't"]
    negated_tokens = []
    negation_flag = False
    negation_phrase = []

    for token in tokens:
        if any(neg in token for neg in negation_words):
            negation_flag = True  # Start tagging the negated part
            negation_phrase = [token]
        elif negation_flag:
            negation_phrase.append(token)
            if token in [".", "!", "?", ";", ","]:  # End of the negated phrase
                negation_flag = False
                negated_tokens.append("NEG_" + " ".join(negation_phrase))
                negation_phrase = []
        else:
            negated_tokens.append(token)

    return " ".join(negated_tokens)

# Define neutral sentiment keywords
neutral_keywords = ["okay", "alright", "decent", "average", "indifferent", "neither good nor bad", "not sure", "pretty average", "nothing special", "meh", "it's fine"]

# Label neutral sentiment
def label_neutral(text):
    for word in neutral_keywords:
        if word in text:
            return "neutral"
    return None

# Applying preprocessing to the dataframe
# Assuming `df` is your dataframe with 'sentiment' and 'text' columns
df['text'] = df['text'].apply(lambda x: x.lower())  # Convert to lowercase
df['text'] = df['text'].apply(remove_urls)  # Remove URLs
df['text'] = df['text'].apply(remove_html_tags)  # Remove HTML tags
df['text'] = df['text'].apply(remove_special_chars)  # Remove special characters
df['text'] = df['text'].apply(remove_retweet_prefix)  # Remove RT prefix

# Apply negation handling after text cleaning
df['text'] = df['text'].apply(handle_negation)

# Label neutral sentiments
df['sentiment'] = df.apply(lambda row: label_neutral(row['text']) if label_neutral(row['text']) else row['sentiment'], axis=1)

# Preview the dataframe to ensure everything is applied
print(df.head())

# prompt: sentiment value count

sentiment_counts = df['sentiment'].value_counts()
sentiment_counts

print(df['text'])

# Plot word clouds for each sentiment
stopwords = set(STOPWORDS)
# Increased figure size for better visualization with more subplots
fig = plt.figure(figsize=(20, 20)) # Increased figure size
num_sentiments = len(df['sentiment'].unique())
# Calculate the number of rows and columns for the subplot grid
num_cols = 2  # You can adjust this if needed
num_rows = (num_sentiments + num_cols - 1) // num_cols

for index, sentiment in enumerate(df['sentiment'].unique()):
    # Create subplots dynamically based on the number of sentiments
    plt.subplot(num_rows, num_cols, index + 1)
    df_subset = df[df['sentiment'] == sentiment]
    wordcloud = WordCloud(
        background_color='white',
        stopwords=stopwords,
        max_words=500,
        max_font_size=40,
        scale=5
    ).generate(' '.join(df_subset['text']))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(sentiment, fontsize=20)
    plt.axis('off')

plt.tight_layout()
plt.show()

# Model building
X = df['text']
y = df['sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = Pipeline([
    ('tfidf', TfidfVectorizer(stop_words='english')),  # Use 'english' for built-in stop words
    ('clf', RandomForestClassifier(n_estimators=100,class_weight='balanced', n_jobs=-1))
])
clf.fit(X_train, y_train)

# Evaluation
predictions = clf.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, predictions):.2f}")
print(classification_report(y_test, predictions))



test_sentences = [
    "I do not play game",  # Negative due to negation
    "It's okay",            # Neutral
    "I love this game",     # Positive
    "I hate this game",     # Negative
    "It's decent",          # Neutral
    "The game is awesome",  # Positive
    "The game is terrible", # Negative
    "This game is alright", # Neutral
]

predictions = clf.predict(test_sentences)
print(predictions)  # Check if the output is as expected

# prompt: NEXT STEP

# Predict sentiment for new text
new_text = new_tweets = [
    "I absolutely love this game, it's amazing!",
    "This game is a complete disaster, I regret buying it.",
    "The game is okay, nothing special.",
    "Best game ever! Highly recommend it to everyone!",
    "The worst experience I've ever had with a game, don't waste your money!",
    "This game is incredibly fun and engaging.",
    "I'm so disappointed with this game, it's buggy and frustrating.",
    "It's a decent game, but not groundbreaking.",
    "I can't stop playing this game, it's addictive!",
    "I wouldn't  this game to anyone."

]
new_predictions = clf.predict(new_text)
new_predictions

# prompt: PREDICT SOME NEUTRAL TWEETS

# Predict sentiment for new neutral text
new_neutral_tweets = [
    "The weather today is neither good nor bad.",
    "I'm not sure how I feel about this movie.",
    "This is a pretty average restaurant.",
    "The game was alright, nothing special.",
    "I'm indifferent towards this product.",
    "It's an okay day.",
    "The performance was adequate.",
    "I have mixed feelings about this book.",
    "This is a neutral statement.",
    "I don't have a strong opinion about this topic."
]

new_neutral_predictions = clf.predict(new_neutral_tweets)
new_neutral_predictions

# prompt: predicte more with simple and moderate tweets

# Predict sentiment for new text
new_tweets = [
    "I absolutely love this game, it's amazing!",
    "This game is a complete disaster, I regret buying it.",
    "The game is okay, nothing special.",
    "Best game ever! Highly recommend it to everyone!",
    "The worst experience I've ever had with a game, don't waste your money!",
    "This game is incredibly fun and engaging.",
    "I'm so disappointed with this game, it's buggy and frustrating.",
    "It's a decent game, but not groundbreaking.",
    "I can't stop playing this game, it's addictive!",
    "I wouldn't recommend this game to anyone.", "The weather today is neither good nor bad.",
    "I'm not sure how I feel about this movie.",
    "This is a pretty average restaurant.",
    "The game was alright, nothing special.",
    "I'm indifferent towards this product.",
    "It's an okay day.",
    "The performance was adequate.",
    "I have mixed feelings about this book.",
    "This is a neutral statement.",
    "I don't have a strong opinion about this topic."

]
new_predictions = clf.predict(new_tweets)
new_predictions

test_sentences = [
    "I can play this game",
    "I cannot play this game",
    "I wouldn't buy this game",
    "The game is neutral",
    "Game is okay",
    "Average play",
    "I love to play this game",
    "This game is not bad",
    "I hate to play this game",
    "I enjoy this game a lot",
    "This game is frustrating",
    "The experience is decent",
    "This is good.",
    "This is bad.",
    "This is ok.",
    "I feel great!",
    "I'm so sad.",
    "It's alright.",
    "Meh, it's just there.",
    "I'm feeling neutral about this.",
    "I neither like nor dislike it.",
    "Pretty decent experience.",
    "Absolutely fantastic!",
    "Dreadful, terrible, awful!"
]

predictions = clf.predict(test_sentences)
print(predictions)


predictions = clf.predict(test_sentences)
print(predictions)  # Check if the output is as expected

# prompt: use some cases from x.test for prediction  set

# Predict sentiment for the test sentences
predictions = clf.predict(test_sentences)
predictions



# prompt: testing some input from userfor testing model

# Predict sentiment for user input
user_input = input("Enter a sentence: ")
user_prediction = clf.predict([user_input])
print(f"Predicted sentiment: {user_prediction[0]}")

# prompt: predict some negattive tweets

# Predict sentiment for new negative text
new_negative_tweets = [" i will never buy this game", "i will never do it again"
    "I absolutely hate this game, it's terrible!",
    "This game is a complete waste of time and money.",
    "The game is buggy, frustrating, and incredibly boring.",
    "Worst game ever! I wouldn't recommend it to my worst enemy!",
    "The worst experience I've ever had with a game, it's unplayable!",
    "This game is incredibly disappointing and uninspired.",
    "I'm so disappointed with this game, it's a disaster.",
    "It's a terrible game, don't waste your money.",
    "I can't stand playing this game, it's addictive in a bad way!",
    "I wouldn't recommend this game to anyone, it's awful!"
]

new_negative_predictions = clf.predict(new_negative_tweets)
new_negative_predictions

# prompt: predict some positive tweet

# Predict sentiment for new positive text
new_positive_tweets = [
    "I absolutely love this game, it's amazing!",
    "This game is fantastic, I highly recommend it",
    "The game is incredibly fun and engaging.",
    "Best game ever! Highly recommend it to everyone!",
    "This game is incredibly fun and engaging.",
    "I can't stop playing this game, it's addictive!",
    "I love everything about this game."
    "i can play game ", "i should do it again","I would recommend this game to anyone",
]

new_positive_predictions = clf.predict(new_positive_tweets)
new_positive_predictions

import pickle
from sklearn.ensemble import RandomForestClassifier



!pip install pickle-mixin

with open('RandomForestClassifier.pkl', 'wb') as file:
    pickle.dump(clf, file)

# prompt: download vectoerizer

import pickle

# ... (your existing code) ...

# Assuming 'clf' is your trained pipeline
with open('RandomForestClassifier.pkl', 'wb') as file:
    pickle.dump(clf, file)

# To load the vectorizer later:
# with open('RandomForestClassifier.pkl', 'rb') as file:
#     loaded_clf = pickle.load(file)

# prompt:  download this  Tfidfvectorizer.pkl

import pickle
with open('Tfidfvectorizer.pkl', 'wb') as file:
    pickle.dump(clf['tfidf'], file)

# prompt: download model to local pc

from google.colab import files
files.download('RandomForestClassifier.pkl')
files.download('Tfidfvectorizer.pkl')